%YAML 1.2
---
KUBE:
   HOME: 
        path: /home/jonarbo/Devel/Kube/KUBE-097

   RUNS:  
        path: var/runs          # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to HOME ([KUBE][HOME])  will be used       
        lifespan:  1             # duration in days (integer value) ... if 0 keep runs results forever ... 

   RESULTS: 
        path: var/results      # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to HOME ([KUBE][HOME])  will be used       

   TOOLS: 
        path: tools             # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to HOME ([KUBE][HOME])  will be used

   RRDTOOL:     
        root: /home/jonarbo/Bin/rrdtool          # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to HOME ([KUBE][HOME])  will be used
        path: var/rrd                            # path to where the database will be stored

   BATCH:             
        - name: slurm-dalma
          script: run.slurm.in # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to [KUBE][HOME]/etc  will be used          
          submit: 
                command: sbatch 
                parameters: ' '   
                submittedmsg: Submitted batch job %JOBID% 
          monitor: squeue | gawk '{print $1}' | grep %JOBID%
          numprocs: 28,56,84
          tasks_per_node: 28           
          queue: ' '                             
          launcher: srun 
          launcher_flags: ' '     
          wallclock: ' '       
        
        - name: LSF
          script: run.lsf.in # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to [KUBE][HOME]/etc  will be used          
          submit: 
                command: bsub 
                parameters:  < 
                submittedmsg: Job <%JOBID%> is submitted to queue 
          monitor: bjobs -u all | gawk '{print $1}' | grep %JOBID%
          numprocs: 4, 8,16,32
          tasks_per_node: 8           
          queue: rh6_q20m                             
          launcher: mpirun 
          launcher_flags: ' '     
          #mpiflags: -a openmpi
          wallclock: 20       
                
        # If no batch system exists and need to run manually, the user must provide a runnable script 
        # either here or in the dataset ato use it to launch the binary. 
        # the 'command' tag has to be 'run.batch' and it will contain what is inside the 'script' plus
        # the vars substitution 
        - name: manual-parallel 
          #script: run.manual.parallel.in  # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to [KUBE][HOME]/etc  will be used          
          script: ' ' # leave it blank ... Will be defined later on the dataset
          submit:
                command: run.batch     # For any manual submission, this line HAVE TO BE THIS
                parameters: ' '  
                submittedmsg: "%JOBID%" 
          launcher: mpirun 
          hostsfile: hostslist
          #launcher_flags: '-np %NUMPROCS% --machinefile=%HOSTSFILE% '     
          launcher_flags: '-np %NUMPROCS%'     
          modules: module load openmpi 
          monitor: ps -fea | grep  %JOBID% | grep -v grep 

        - name: manual-serial 
          script: ' ' # leave it blank ... Will be defined later on the dataset
          submit:
                command: run.batch     # For any manual submission, this line HAVE TO BE THIS
                parameters: ' '  
                submittedmsg: "%JOBID%" 
          launcher: ' '
          launcher_flags: ' '     
          monitor: ps -fea | grep  %JOBID% | grep -v grep 

   BENCH: 
      APPS:
         - name: namd
           active: true
           batch: manual-parallel    
           script: run.manual.in         
           #batch: slurm-dalma    
           numprocs: 60
           # following parameters are dataset dependent... 
           # This values will be used unless they are redefined in the "dataset" section 
           #exe: namd2
           exe: /share/apps/NYUAD/namd/openmpi_1.10.2/avx2/2.10/bin/namd2     
           #modules:  module load namd    
           metrics: 
              - name: total_energy
                units: joules
                command: cat %OUTPUT% |  grep  ENERGY | gawk 'END{print $12}'   
                reference:
                        value: cat %OUTREF% |  grep  ENERGY | gawk 'END{print $12}'
                        tolerance: bilateral  
                        threshold: 0.000001
                        threshold_type: percentage        
              - name: wallclock
                units: secs
                command: cat %OUTPUT% | grep WallClock | gawk '{print $2}'                
              - name: days_per_ns
                units: days/ns
                command: cat %OUTPUT% | grep  Benchmark | gawk 'END{print $8}' 
                reference:                                      # Used to check if this metric is within the accepted threshold compared with the ref value
                        value: cat %OUTREF% | grep  Benchmark | gawk 'END{print $8}'  # Number or expression
                        tolerance: above                                              # above/below/bilateral       
                        threshold: inf                                                # Number or "Inf"      
                        threshold_type:  absolute                                     # absoulte/percentage
              - name: cputime
                units: secs
                command: cat %OUTPUT% | grep CPUTime | gawk '{print $4}'                     
              - name: memory
                units: MB
                command: cat %OUTPUT%  | grep Memory | gawk '{print $6}'         
              - name: prueba
                units: Julios
                command:  /%COMMON%/printCommonTolerance.sh %OUTPUT% %OUTREF%
         
           # here goes the specific dataset configuration
           datasets: 
              - name: namd_nucleosome146Katoms
                active: true
                args: nucleosome146Katoms.namd  > nucleosome.out  2>nucleosome.err
                outputs:  
                   output: nucleosome.out
                   #outref: nucleosome.ref
                   another_output: KUBE_%NAME%_%NUMPROCS%.out          
    
              - name: namd_apoa1  
                active: false
                numprocs: 4, 8 ,16
                args:  apoa1.namd   > apoa1.out 2>apoa1.err                        
                outputs: 
                   output: apoa1.out  
                   outref: apoa1.ref
                   another_output: KUBE_%NAME%_%NUMPROCS%.out          

         - name: vasp
           active: false
           batch: LSF
           exe: vasp         
           modules:  module load gcc mpi-openmpi vasp
           datasets:
              - name: vasp_Hg
                active: true
                args: 
                outputs: 
                   outcar: OUTCAR
                   oszicar: OSZICAR
                   outcar_ref:  OUTCAR.ref
                   oszicar_ref: OSZICAR.ref
                metrics: 
                   - name: wallclock
                     units: secs
                     command: $cat %OUTCAR% | grep "Elapsed time" | gawk '{print $4}' 

         - name: siesta
           active: false
           batch: LSF
           exe: siesta
           modules:  module load gcc mpi-openmpi blas blacs  lapack scalapack siesta
           datasets:
              - name: siesta_test
                active: true
                args: < fen.fdf  > output.txt 
                outputs:
                   clock: CLOCK
                   output: output.txt
                   fenxml: fen.xml
                   feneig: fen.EIG
                metrics: 
                   - name: wallclock
                     units: secs
                     command: cat %CLOCK% | grep "End of run" | gawk '{print $4}'
                   - name: AvgMem # Avg mem per node
                     units: MB
                     command: cat %OUTPUT% | grep "Node" | gawk '{print $10}' |  gawk 'BEGIN{sum=0 ; c=0}{ sum=sum+$1 ; c=c+1  }END{ print sum/c}'  
                   - name: FinalEnergy
                     units: eV
                     command:  cat %OUTPUT%  | grep "Total =" | gawk '{print $4}' 

      FILESYSTEMS: 
         - name: gpfs_iozone3
           active: false
           batch: None
           numprocs: 1
           modules:  module load gcc mpi-openmpi
           datasets:
                - name: iozone3_408
                  args: -Rab output.wks -g 500M  -I | tee iozone.out
                  exe: bin/iozone
                  dependencies: bin/* 
                  active: true
                  outputs:
                    iozone: iozone.out
                    output: output.wks

         - name: gpfs_mpi_io
           active: false
           batch: LSF
           modules:  module load gcc mpi-openmpi
           numprocs: 16
           exe: ./fs_test.x
           tmpfilepath: /scratch/naranjja/mpiio_testfile_%NAME%
           metrics:
             - name: Efective_BW_Write_avg
               units: MB/s
               command: cat  %OUTPUT% | grep  "Effective Bandwidth"  | gawk '{print $11}'  | head -n 1
             - name: BW_Write_avg
               units: MB/s
               command: cat  %OUTPUT% | grep  "Write Bandwidth"  | gawk '{print $11}'  | head -n 1
             - name: MPI_FileOpen_Write_avg
               units: secs
               command: cat  %OUTPUT% | grep  "MPI_File_Write_Open"  | gawk '{print $11}'  | head -n 1
             - name: Efective_BW_Read_avg
               units: MB/s
               command:  cat  %OUTPUT% | grep  "Effective Bandwidth"  | gawk '{print $11}'  | tail  -n 1
             - name: BW_Read_avg
               units: MB/s
               command: cat  %OUTPUT% | grep  "Write Bandwidth"  | gawk '{print $11}'  | tail -n 1
             - name: MPI_FileOpen_Read_avg
               units: secs
               command: cat  %OUTPUT% | grep  "MPI_File_Write_Open"  | gawk '{print $11}'  | tail -n 1
           outputs:
             output: KUBE_%NAME%_%NUMPROCS%.out 
           datasets:
                - name: mpi_io_type2_Nto1_noStrided  # MPI-IO on Scratch N procs to 1 file
                  active: true                
                  # 10 objects of 1GB to write by each proc 
                  args:  -type 2 -strided 0 -nobj 10 -size 1073741824  -target %TMPFILEPATH% -touch 2 -check 2
                
                - name: mpi_io_type1_NtoN_noStrided  # MPI-IO on Scratch N procs to 1 file
                  active: true
                  # 10 objects of 1GB to write by each proc 
                  args:  -type 1 -strided 0 -nobj 10 -size 1073741824  -target  %TMPFILEPATH%  -touch 2 -check 2

      NETWORKS: 
         - name: infiniband_mpi
           batch: slurm-dalma         
           active: false
           modules:   module load gcc   
           numprocs:  240         
           datasets:           
                - name: skampi-5.0.4-r0355
                  active: false
                  exe: ./skampi
                  numprocs: 32
                  dependencies:  ski/*.inc , ski/skampi_coll.ski 
                  args:  -i ski/skampi_coll.ski  -o skaoutf.txt
                  outputs:
                    output: skaoutf.txt    
                    batchout:  KUBE_%NAME%_%NUMPROCS%.out 
                    batcherr:  KUBE_%NAME%_%NUMPROCS%.err 
                                                              
                - name: imb_3.2.3                            
                  active: true  
                  exe: bin/IMB-MPI1 # Full path or  Relative to the current dir
                  dependencies:   #comma separated list of  files that will be copied to the run directory ... always relative to the 'source' dir
                  args: 1>imb.output                                       
                  outputs: 
                    output: imb.output
                  metrics:
                    - name: pingpong_BW
                      units: MB/secs
                      command: cat %OUTPUT% | grep -A28 "Benchmarking PingPong" |  gawk '$1 ~ /[0-9]/'  | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$4  }END{ print sum/8}'
                    - name: pingping_BW
                      units: MB/secs
                      command: cat %OUTPUT% | grep -A28 "Benchmarking PingPing" |  gawk '$1 ~ /[0-9]/'  | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$4  }END{ print sum/8}'       
                    - name: sendrecv_BW
                      units: MB/secs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Exchange" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$6  }END{ print sum/8}'
                    - name: exchange_BW
                      units: MB/secs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Allreduce" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$6  }END{ print sum/8}'
                    - name: allreduce_avg_t
                      units: usecs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Reduce" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$5  }END{ print sum/8}'
                    - name: alltoall_avg_t
                      units: usecs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Bcast" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$5  }END{ print sum/8}'
                    - name: bcast_avg_t 
                      units: usecs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Barrier" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$5  }END{ print sum/8}'

      SYNTHETICS:
         - name: hpl-linpack
           active: false
           batch: slurm-dalma         
           exe: ./xhpl 
           args: 
           modules: 
           dependencies: hpccinf.txt
           outputs:
             hpccout: hpccoutf.txt
           metrics:
               - name: Procs
                 units: ""
                 command: cat  %HPCCOUT% | grep CommWorldProcs | gawk -F "=" '{print $2}'
               - name: HPL
                 units: TFlops
                 command: cat  %HPCCOUT% | grep HPL_Tflops | gawk -F "=" '{print $2}'
               - name: PTRANS
                 units: GB/s
                 command:  cat  %HPCCOUT% | grep  PTRANS_GBs | gawk -F "=" '{print $2}'
               - name: MPIRandomAccess
                 units: GUPs
                 command:  cat  %HPCCOUT% | grep  MPIRandomAccess_GUPs | gawk -F "=" '{print $2}'                                  
               - name: SingleSTREAM_Triad
                 units: GB/s
                 command: cat  %HPCCOUT% | grep  SingleSTREAM_Triad | gawk -F "=" '{print $2}'                                          
               - name: FFT
                 units: GFlop/s
                 command: cat  %HPCCOUT% | grep  MPIFFT_Gflops | gawk -F "=" '{print $2}'                                          
               - name: Single_DGEMM
                 units: GFlop/s
                 command: cat  %HPCCOUT% | grep  SingleDGEMM_Gflops| gawk -F "=" '{print $2}'
               - name: Random_RingBandwidth
                 units: GB/s
                 command: cat  %HPCCOUT% | grep  RandomlyOrderedRingBandwidth_GBytes | gawk -F "=" '{print $2}'
               - name: Random_RingLatency
                 units: usecs
                 command: cat  %HPCCOUT% | grep  RandomlyOrderedRingLatency_usec  | gawk -F "=" '{print $2}'           
           datasets:
                 - name: with_atlas_32cpus
                   numprocs: 32
                   active: true    
                                  
                 - name: with_atlas_64cpus
                   numprocs: 64
                   active: true                
...
