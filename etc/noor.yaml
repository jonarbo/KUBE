%YAML 1.2
---
KUBE:
   HOME: 
        path: /home/naranjja/Devel/KUBE

   RUNS:  
        path: results/runs          # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to HOME ([KUBE][HOME])  will be used       
        lifespan:  1                # duration in days (integer value) ... if 0 keep runs results forever ... 

   ANALYSIS: 
        path: results/analysis      # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to HOME ([KUBE][HOME])  will be used       

   TOOLS: 
        path: tools                 # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to HOME ([KUBE][HOME])  will be used

   BATCH:             
        - name: LSF
          script: run.lsf.in # if the leading "/" is provided then the absolute path is taken, otherwise the relative path to [KUBE][HOME]/etc  will be used          
          submit: 
                command: bsub 
                parameters:  < 
                submittedmsg: Job <%JOBID%> is submitted to queue 
          monitor: bjobs -u all | gawk '{print $1}' | grep %JOBID%
          numprocs: 4, 8,16,32
          tasks_per_node: 8           
          queue: rh6_q20m                             
          launcher: mpirun 
          launcher_flags: ' '     
          #mpiflags: -a openmpi
          wallclock: 20       
                
        # for a  manual launcher the submission command will be: <command> <parameters> <exe> <args>
        # variables %XXX% specific to the dataset will be replaced at the moment the script is generated
        # Other %XXX% variables can be used in the 'submit' section if they are defined..ie: %hostslistfile% 
        - name: MANUAL
          hostslistfile: hostnames      
          submit: 
                command: mpirun 
                parameters: -np %NUMPROCS% -machinefile %HOSTSLISTFILE%  

   BENCH: 
      APPS:
         - name: mpiblast
           active: true 
           batch: LSF 
           exe: mpiblast 
           modules: module load gcc mpi-openmpi mpiblast
           datasets:
                - name: yeast 
                  active: true
                  numprocs: 4,6,8
                  args: -p blastn -d yeast.nt -i query.txt -o mpiblastresult.txt 
                  outputs:
                        outputtxt: mpiblastresult.txt
                  metrics:
                        - name: sequences
                          units: number
                          command: cat %OUTPUTTXT% | grep -A1 Database | grep sequences |  gawk '{print $1}' 
                        - name: total_letters
                          units: number
                          command: cat %OUTPUTTXT% | grep -A1 Database | grep sequences |  gawk '{print $3}' | sed 's/,//g'
         - name: namd
           active: true
           batch: LSF     
           # following parameters are dataset dependent... 
           # This values will be used unless they are redefined in the "dataset" section 
           exe: namd2_mpi
           modules:  module load gcc mpi-openmpi  libs-extra fftw2/2.1.5-openmpi-1.4.3-sse4.2-sp namd    
           metrics: 
              - name: total_energy
                units: joules
                command: cat %OUTPUT% |  grep  ENERGY | gawk 'END{print $12}'   
                reference:
                        value: cat %OUTREF% |  grep  ENERGY | gawk 'END{print $12}'
                        tolerance: bilateral  
                        threshold: 0.000001
                        threshold_type: percentage        
              - name: wallclock
                units: secs
                command: cat %OUTPUT% | grep WallClock | gawk '{print $2}'                
              - name: days_per_ns
                units: days/ns
                command: cat %OUTPUT% | grep  Benchmark | gawk 'END{print $8}' 
                reference:                                      # Used to check if this metric is within the accepted threshold compared with the ref value
                        value: cat %OUTREF% | grep  Benchmark | gawk 'END{print $8}'  # Number or expression
                        tolerance: above                                              # above/below/bilateral       
                        threshold: inf                                                # Number or "Inf"      
                        threshold_type:  absolute                                     # absoulte/percentage
              - name: cputime
                units: secs
                command: cat %OUTPUT% | grep CPUTime | gawk '{print $4}'                     
              - name: memory
                units: MB
                command: cat %OUTPUT%  | grep Memory | gawk '{print $6}'         
              - name: prueba
                units: Julios
                command:  /%COMMON%/printCommonTolerance.sh %OUTPUT% %OUTREF%

           # here goes the specific dataset configuration
           datasets: 
              - name: namd_nucleosome146Katoms
                active: true
                args: nucleosome146Katoms.namd  > nucleosome.out  2>nucleosome.err
                outputs:  
                   output: nucleosome.out                      
                   outref: nucleosome.ref
                   another_output: KUBE_%NAME%_%NUMPROCS%.out          
    
              - name: namd_apoa1  
                active: true
                numprocs: 4, 8 ,16
                args:  apoa1.namd   > apoa1.out 2>apoa1.err                        
                outputs: 
                   output: apoa1.out  
                   outref: apoa1.ref
                   another_output: KUBE_%NAME%_%NUMPROCS%.out          

         - name: vasp
           active: false
           batch: LSF
           exe: vasp         
           modules:  module load gcc mpi-openmpi vasp
           datasets:
              - name: vasp_Hg
                active: true
                args: 
                outputs: 
                   outcar: OUTCAR
                   oszicar: OSZICAR
                   outcar_ref:  OUTCAR.ref
                   oszicar_ref: OSZICAR.ref
                metrics: 
                   - name: wallclock
                     units: secs
                     command: $cat %OUTCAR% | grep "Elapsed time" | gawk '{print $4}' 

         - name: siesta
           active: true
           batch: LSF
           exe: siesta
           modules:  module load gcc mpi-openmpi blas blacs  lapack scalapack siesta
           datasets:
              - name: siesta_test
                active: true
                args: < fen.fdf  > output.txt 
                outputs:
                   clock: CLOCK
                   output: output.txt
                   fenxml: fen.xml
                   feneig: fen.EIG
                metrics: 
                   - name: wallclock
                     units: secs
                     command: cat %CLOCK% | grep "End of run" | gawk '{print $4}'
                   - name: AvgMem # Avg mem per node
                     units: MB
                     command: cat %OUTPUT% | grep "Node" | gawk '{print $10}' |  gawk 'BEGIN{sum=0 ; c=0}{ sum=sum+$1 ; c=c+1  }END{ print sum/c}'  
                   - name: FinalEnergy
                     units: eV
                     command:  cat %OUTPUT%  | grep "Total =" | gawk '{print $4}' 

      FILESYSTEMS: 
         - name: gpfs_iozone3
           active: true
           batch: None
           numprocs: 1
           modules:  module load gcc mpi-openmpi
           datasets:
                - name: iozone3_408
                  args: -Rab output.wks -g 500M  -I | tee iozone.out
                  exe: bin/iozone
                  dependencies: bin/* 
                  active: true
                  outputs:
                    iozone: iozone.out
                    output: output.wks

         - name: gpfs_mpi_io
           active: false
           batch: LSF
           modules:  module load gcc mpi-openmpi
           numprocs: 16
           exe: ./fs_test.x
           tmpfilepath: /scratch/naranjja/mpiio_testfile_%NAME%
           metrics:
             - name: Efective_BW_Write_avg
               units: MB/s
               command: cat  %OUTPUT% | grep  "Effective Bandwidth"  | gawk '{print $11}'  | head -n 1
             - name: BW_Write_avg
               units: MB/s
               command: cat  %OUTPUT% | grep  "Write Bandwidth"  | gawk '{print $11}'  | head -n 1
             - name: MPI_FileOpen_Write_avg
               units: secs
               command: cat  %OUTPUT% | grep  "MPI_File_Write_Open"  | gawk '{print $11}'  | head -n 1
             - name: Efective_BW_Read_avg
               units: MB/s
               command:  cat  %OUTPUT% | grep  "Effective Bandwidth"  | gawk '{print $11}'  | tail  -n 1
             - name: BW_Read_avg
               units: MB/s
               command: cat  %OUTPUT% | grep  "Write Bandwidth"  | gawk '{print $11}'  | tail -n 1
             - name: MPI_FileOpen_Read_avg
               units: secs
               command: cat  %OUTPUT% | grep  "MPI_File_Write_Open"  | gawk '{print $11}'  | tail -n 1
           outputs:
             output: KUBE_%NAME%_%NUMPROCS%.out 
           datasets:
                - name: mpi_io_type2_Nto1_noStrided  # MPI-IO on Scratch N procs to 1 file
                  active: true                
                  # 10 objects of 1GB to write by each proc 
                  args:  -type 2 -strided 0 -nobj 10 -size 1073741824  -target %TMPFILEPATH% -touch 2 -check 2
                
                - name: mpi_io_type1_NtoN_noStrided  # MPI-IO on Scratch N procs to 1 file
                  active: true
                  # 10 objects of 1GB to write by each proc 
                  args:  -type 1 -strided 0 -nobj 10 -size 1073741824  -target  %TMPFILEPATH%  -touch 2 -check 2

      NETWORKS: 
         - name: infiniband_mpi
           batch: LSF         
           active: true
           modules:  module load gcc mpi-openmpi    
           numprocs:  16,32,64         
           datasets:           
                - name: skampi-5.0.4-r0355
                  active: false
                  exe: ./skampi
                  numprocs: 32
                  dependencies:  ski/*.inc , ski/skampi_coll.ski 
                  args:  -i ski/skampi_coll.ski  -o skaoutf.txt
                  outputs:
                    output: skaoutf.txt    
                    batchout:  KUBE_%NAME%_%NUMPROCS%.out 
                    batcherr:  KUBE_%NAME%_%NUMPROCS%.err 
                                                              
                - name: imb_3.2.3                            
                  active: true  
                  exe: bin/IMB-MPI1 # Full path or  Relative to the current dir
                  dependencies:   #comma separated list of  files that will be copied to the run directory ... always relative to the 'source' dir
                  args: 1>imb.output                                       
                  outputs: 
                    output: imb.output
                  metrics:
                    - name: pingpong_BW
                      units: MB/secs
                      command: cat %OUTPUT% | grep -A28 "Benchmarking PingPong" |  gawk '$1 ~ /[0-9]/'  | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$4  }END{ print sum/8}'
                    - name: pingping_BW
                      units: MB/secs
                      command: cat %OUTPUT% | grep -A28 "Benchmarking PingPing" |  gawk '$1 ~ /[0-9]/'  | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$4  }END{ print sum/8}'       
                    - name: sendrecv_BW
                      units: MB/secs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Exchange" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$6  }END{ print sum/8}'
                    - name: exchange_BW
                      units: MB/secs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Allreduce" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$6  }END{ print sum/8}'
                    - name: allreduce_avg_t
                      units: usecs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Reduce" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$5  }END{ print sum/8}'
                    - name: alltoall_avg_t
                      units: usecs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Bcast" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$5  }END{ print sum/8}'
                    - name: bcast_avg_t 
                      units: usecs
                      command: cat %OUTPUT% | grep -B10  "Benchmarking Barrier" | head | gawk 'BEGIN{sum=0}{if ($1>=32768) sum=sum+$5  }END{ print sum/8}'

      SYNTHETICS:
         - name: NAS
           active: true
           batch: LSF 
           numprocs: 4 , 8
           modules:  module load gcc mpi-openmpi
           dependencies: bin/*.%NUMPROCS%
           datasets:
                 - name: NPB3.3-MPI
                   active: true
                   exe: ./bin/mg.A.%NUMPROCS%
                   args:  1>mg.A.%NUMPROCS%.out 2>mg.A.%NUMPROCS%.err
                   outputs:
                     output: mg.A.%NUMPROCS%.out
                     outputlsf: KUBE_%NAME%_%NUMPROCS%.out 
                     error:  KUBE_%NAME%_%NUMPROCS%.err          

         - name: hpcc
           active: true
           batch: LSF         
           exe: ./hpcc 
           args: 
           modules:  module load gcc atlas/3.8.4-sse3 mpi-openmpi   
           dependencies: hpccinf.txt
           outputs:
             hpccout: hpccoutf.txt
           metrics:
               - name: Procs
                 units: ""
                 command: cat  %HPCCOUT% | grep CommWorldProcs | gawk -F "=" '{print $2}'
               - name: HPL
                 units: TFlops
                 command: cat  %HPCCOUT% | grep HPL_Tflops | gawk -F "=" '{print $2}'
               - name: PTRANS
                 units: GB/s
                 command:  cat  %HPCCOUT% | grep  PTRANS_GBs | gawk -F "=" '{print $2}'
               - name: MPIRandomAccess
                 units: GUPs
                 command:  cat  %HPCCOUT% | grep  MPIRandomAccess_GUPs | gawk -F "=" '{print $2}'                                  
               - name: SingleSTREAM_Triad
                 units: GB/s
                 command: cat  %HPCCOUT% | grep  SingleSTREAM_Triad | gawk -F "=" '{print $2}'                                          
               - name: FFT
                 units: GFlop/s
                 command: cat  %HPCCOUT% | grep  MPIFFT_Gflops | gawk -F "=" '{print $2}'                                          
               - name: Single_DGEMM
                 units: GFlop/s
                 command: cat  %HPCCOUT% | grep  SingleDGEMM_Gflops| gawk -F "=" '{print $2}'
               - name: Random_RingBandwidth
                 units: GB/s
                 command: cat  %HPCCOUT% | grep  RandomlyOrderedRingBandwidth_GBytes | gawk -F "=" '{print $2}'
               - name: Random_RingLatency
                 units: usecs
                 command: cat  %HPCCOUT% | grep  RandomlyOrderedRingLatency_usec  | gawk -F "=" '{print $2}'           
           datasets:
                 - name: with_atlas_32cpus
                   numprocs: 32
                   active: true    
                                  
                 - name: with_atlas_64cpus
                   numprocs: 64
                   active: true                
...
